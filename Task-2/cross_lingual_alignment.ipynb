{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "Firstly, we download pre-trained embeddings in English and Hindi and the bilingual dictionary from MUSE and prepare the data for alignment"
      ],
      "metadata": {
        "id": "ZG1Nb-ClhEPW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcdM6HH5eygx",
        "outputId": "ff2d6c71-3f0f-4f69-a9e0-3bd18a7b85bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-18 15:22:58--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.102, 108.157.254.121, 108.157.254.124, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G   114MB/s    in 9.8s    \n",
            "\n",
            "2025-04-18 15:23:08 (129 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download pre-trained English word embeddings\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download pre-trained Hindi word embeddings\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr8mKbv5fLfD",
        "outputId": "0504703a-e58d-40d4-9df5-c2063f5fa55e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-18 15:23:38--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.124, 108.157.254.15, 108.157.254.102, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1118942272 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘cc.hi.300.vec.gz’\n",
            "\n",
            "cc.hi.300.vec.gz    100%[===================>]   1.04G  15.6MB/s    in 67s     \n",
            "\n",
            "2025-04-18 15:24:46 (15.9 MB/s) - ‘cc.hi.300.vec.gz’ saved [1118942272/1118942272]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load top 100,000 embeddings from English and Hindi in decreasing order of frequency."
      ],
      "metadata": {
        "id": "ABYl_zqqgY-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the embeddings\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "def load_embeddings(file_path, top_n=100000):\n",
        "    embeddings = {}\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            if i > top_n:\n",
        "                break\n",
        "            tokens = line.decode('utf-8').strip().split(' ')\n",
        "            word = tokens[0]\n",
        "            vector = np.array(tokens[1:], dtype=np.float32)\n",
        "            vector = vector / np.linalg.norm(vector)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "en_embeddings = load_embeddings('cc.en.300.vec.gz', top_n=100000)\n",
        "hi_embeddings = load_embeddings('cc.hi.300.vec.gz', top_n=100000)\n",
        "\n",
        "print(f\"Loaded {len(en_embeddings)} English embeddings\")\n",
        "print(f\"Loaded {len(hi_embeddings)} Hindi embeddings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_IjgGMlflKP",
        "outputId": "8e840e08-8e6d-44c6-f5d9-7582178347f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100000 English embeddings\n",
            "Loaded 100000 Hindi embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download english-hindi dictionary from MUSE\n",
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCHz0m2Gf8bu",
        "outputId": "794ebaa7-3a93-4595-c6f5-1e7b82e356b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-18 15:25:24--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.124, 108.157.254.121, 108.157.254.102, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930856 (909K) [text/x-c++]\n",
            "Saving to: ‘en-hi.txt’\n",
            "\n",
            "en-hi.txt           100%[===================>] 909.04K  1.13MB/s    in 0.8s    \n",
            "\n",
            "2025-04-18 15:25:26 (1.13 MB/s) - ‘en-hi.txt’ saved [930856/930856]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display English-Hindi pairs\n",
        "def load_bilingual_lexicon(file_path):\n",
        "    bilingual_dict = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            bilingual_dict.append((en_word, hi_word))\n",
        "    return bilingual_dict\n",
        "\n",
        "en_hi_pairs = load_bilingual_lexicon('en-hi.txt')\n",
        "print(en_hi_pairs[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahXbJIfVgS3H",
        "outputId": "3a6c8fd7-8d01-4229-994c-c3a6acfaa66d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('and', 'और'), ('was', 'था'), ('was', 'थी'), ('for', 'लिये'), ('that', 'उस'), ('that', 'कि'), ('with', 'साथ'), ('from', 'से'), ('from', 'इससे'), ('this', 'ये')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract word embeddings for bilingual word pairs\n",
        "import numpy as np\n",
        "\n",
        "def extract_word_embeddings(bilingual_pairs, en_embeddings, hi_embeddings):\n",
        "    en_vecs = []\n",
        "    hi_vecs = []\n",
        "\n",
        "    for en_word, hi_word in bilingual_pairs:\n",
        "        if en_word in en_embeddings and hi_word in hi_embeddings:\n",
        "            en_vecs.append(en_embeddings[en_word])\n",
        "            hi_vecs.append(hi_embeddings[hi_word])\n",
        "\n",
        "    en_vecs = np.array(en_vecs)\n",
        "    hi_vecs = np.array(hi_vecs)\n",
        "\n",
        "    return en_vecs, hi_vecs\n",
        "\n",
        "en_vecs, hi_vecs = extract_word_embeddings(en_hi_pairs, en_embeddings, hi_embeddings)\n",
        "print(f\"Extracted {en_vecs.shape[0]} aligned word vectors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2c3FJZ0g18l",
        "outputId": "b240c5eb-5cc4-4b9c-819a-e4a4a59de9b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 18972 aligned word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Lingual Alignment"
      ],
      "metadata": {
        "id": "cmOp7xbihW7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Procrustes Alignment\n",
        "\n",
        "We perform orthogonal Procrustes alignment to learn a mapping from X to Y, where X is numpy array having source language word embeddings (English) and Y is numpy array having target language word embeddings (Hindi)."
      ],
      "metadata": {
        "id": "qxRjLXzthe7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def orthogonal_procrustes(X, Y):\n",
        "    X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    Y = Y / np.linalg.norm(Y, axis=1, keepdims=True)\n",
        "    M = np.dot(X.T, Y)\n",
        "\n",
        "    U, _, Vt = np.linalg.svd(M)\n",
        "    W = np.dot(U, Vt)\n",
        "\n",
        "    return W\n",
        "\n",
        "W = orthogonal_procrustes(en_vecs, hi_vecs)\n",
        "\n",
        "print(\"Orthogonal mapping matrix learned.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnxrwPCohBCN",
        "outputId": "dd10bac0-4f71-486c-df77-eecdc160e3aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orthogonal mapping matrix learned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply learned mapping to the source language embeddings\n",
        "\n",
        "def apply_mapping(embeddings, W):\n",
        "    mapped_embeddings = {}\n",
        "    for word, vec in embeddings.items():\n",
        "        mapped_vec = np.dot(vec, W)\n",
        "        mapped_vec = mapped_vec / np.linalg.norm(mapped_vec) # normalize the mapped vector\n",
        "        mapped_embeddings[word] = mapped_vec\n",
        "    return mapped_embeddings\n",
        "\n",
        "aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
        "\n",
        "print(f\"Aligned {len(aligned_en_embeddings)} English embeddings into the Hindi space.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imls2ynAhvqi",
        "outputId": "45b5c2bf-4b8e-4cdd-dd79-cf37adb4bccc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aligned 100000 English embeddings into the Hindi space.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "f0-87VNhiF0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Translation\n",
        "\n",
        "We now perform word translation where we translate a limited number of words from English to Hindi using the aligned embeddings. I have limited size of en_words to 2000 since I previously experimented with 3000 words and session crashed terminated after execution of > 2 hours.\n"
      ],
      "metadata": {
        "id": "4YgH2b3DiK6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def translate_words(aligned_en_embeddings, hi_embeddings, top_k=5, limit_size=None):\n",
        "    translations = {}\n",
        "    hi_words = list(hi_embeddings.keys())\n",
        "    hi_vecs = np.array(list(hi_embeddings.values()))\n",
        "\n",
        "    en_words = list(aligned_en_embeddings.keys())\n",
        "    if limit_size is not None:\n",
        "        en_words = en_words[:limit_size]\n",
        "\n",
        "    for en_word in en_words:\n",
        "        en_vec = aligned_en_embeddings[en_word]\n",
        "        en_vec = en_vec / np.linalg.norm(en_vec) # calculate cosine similarity\n",
        "        hi_vecs_norm = hi_vecs / np.linalg.norm(hi_vecs, axis=1, keepdims=True)\n",
        "        similarities = cosine_similarity([en_vec], hi_vecs_norm).flatten()\n",
        "        nearest_idxs = similarities.argsort()[-top_k:][::-1] # get top_k most similar Hindi words\n",
        "        nearest_words = [hi_words[i] for i in nearest_idxs]\n",
        "\n",
        "        translations[en_word] = nearest_words\n",
        "\n",
        "    return translations\n",
        "\n",
        "limit_size = 2000\n",
        "translations = translate_words(aligned_en_embeddings, hi_embeddings, top_k=5, limit_size=limit_size)\n",
        "\n",
        "for en_word, hi_words in list(translations.items())[:10]:\n",
        "    print(f\"English: {en_word} -> Hindi: {hi_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sha5FRivh8Uv",
        "outputId": "4e3f1c9d-ab59-48c3-d22d-aee1b19188ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: , -> Hindi: [',', 'और', 'कि', 'हैं', '।']\n",
            "English: the -> Hindi: ['में', 'पहले', 'सबसे', 'अपने', 'जिस']\n",
            "English: . -> Hindi: ['तो', 'ही', '*', 'आज', '.']\n",
            "English: and -> Hindi: ['तथा', 'साथ', 'एवं', 'और', 'हैं']\n",
            "English: to -> Hindi: ['करके', 'करने', 'करना', 'करें', 'करते']\n",
            "English: of -> Hindi: ['में', 'प्रति', 'तथा', 'सबसे', 'अधीन']\n",
            "English: a -> Hindi: ['बड़ा', 'दूसरा', 'बड़ा', 'पहला', 'छोटा']\n",
            "English: </s> -> Hindi: ['▲', 'è', 'bss', 'pd', '▓']\n",
            "English: in -> Hindi: ['में', 'बाहर', 'जहां', 'मे', 'लाने']\n",
            "English: is -> Hindi: ['है', 'यह', 'होता', 'क्योंकि', 'जो']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation accuracy and Precision using MUSE dictionary\n",
        "\n",
        "We calculate Translation accuracy, P@1 and P@5 to evaluate the aligned embeddings"
      ],
      "metadata": {
        "id": "acuGiy2_l5qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_muse_test_dict(file_path):\n",
        "    test_dict = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            words = line.strip().split()\n",
        "            if len(words) == 2:\n",
        "                test_dict[words[0]] = words[1]\n",
        "    return test_dict"
      ],
      "metadata": {
        "id": "ZRa6t2_xi881"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_translation(translations, test_dict, top_k=5):\n",
        "    true_positives_at_1 = 0\n",
        "    true_positives_at_5 = 0\n",
        "    false_positives_at_1 = 0\n",
        "    false_positives_at_5 = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for en_word, correct_hi_word in test_dict.items():\n",
        "        predicted_hi_words = translations.get(en_word, [])\n",
        "\n",
        "        if len(predicted_hi_words) > 0:\n",
        "            total_predictions += 1\n",
        "\n",
        "            # calculate Precision@1\n",
        "            if correct_hi_word == predicted_hi_words[0]:\n",
        "                true_positives_at_1 += 1\n",
        "                correct_predictions += 1\n",
        "            else:\n",
        "                false_positives_at_1 += 1\n",
        "\n",
        "            # calculate Precision@5\n",
        "            if correct_hi_word in predicted_hi_words[:top_k]:\n",
        "                true_positives_at_5 += 1\n",
        "\n",
        "                # Only count for accuracy if it hasn't been counted for Precision@1\n",
        "                if correct_hi_word != predicted_hi_words[0]:\n",
        "                    correct_predictions += 1\n",
        "            else:\n",
        "                false_positives_at_5 += 1\n",
        "\n",
        "    precision_at_1 = true_positives_at_1 / (true_positives_at_1 + false_positives_at_1) if (true_positives_at_1 + false_positives_at_1) > 0 else 0\n",
        "    precision_at_5 = true_positives_at_5 / (true_positives_at_5 + false_positives_at_5) if (true_positives_at_5 + false_positives_at_5) > 0 else 0\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "    return precision_at_1, precision_at_5, accuracy\n",
        "\n",
        "test_dict = load_muse_test_dict('en-hi.txt')\n",
        "precision_at_1, precision_at_5, accuracy = evaluate_translation(translations, test_dict)\n",
        "\n",
        "print(f\"Precision@1: {precision_at_1:.4f}\")\n",
        "print(f\"Precision@5: {precision_at_5:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5dNbnxlnbZE",
        "outputId": "94f2ca67-4991-4400-e10e-863d0da470c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@1: 0.3827\n",
            "Precision@5: 0.6675\n",
            "Accuracy: 0.6675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine similarity\n",
        "Now, we calculate cosine similarity to measure the similarity between the embeddings in English and in Hindi, thereby getting a quantitative idea about alignment quality"
      ],
      "metadata": {
        "id": "BSEbctpynv-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_cosine_similarities(en_word_pairs, en_embeddings, hi_embeddings, num_pairs=50):\n",
        "    similarities = {}\n",
        "    count = 0\n",
        "\n",
        "    for en_word, hi_word in en_word_pairs:\n",
        "        if en_word in en_embeddings and hi_word in hi_embeddings:\n",
        "            en_vec = en_embeddings[en_word]\n",
        "            hi_vec = hi_embeddings[hi_word]\n",
        "            en_vec = en_vec / np.linalg.norm(en_vec)\n",
        "            hi_vec = hi_vec / np.linalg.norm(hi_vec)\n",
        "            similarity = cosine_similarity([en_vec], [hi_vec])[0][0]\n",
        "            similarities[(en_word, hi_word)] = similarity\n",
        "\n",
        "            count += 1\n",
        "            if count >= num_pairs:\n",
        "                break\n",
        "\n",
        "    return similarities\n",
        "\n",
        "cosine_similarities = compute_cosine_similarities(en_hi_pairs, en_embeddings, hi_embeddings, num_pairs=50) # limit number of pairs to 50\n",
        "for (en_word, hi_word), similarity in cosine_similarities.items():\n",
        "    print(f\"English: {en_word}, Hindi: {hi_word}, Similarity: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8frH25oEnpQY",
        "outputId": "ffa37742-a189-4a78-947c-14bce8e060d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: and, Hindi: और, Similarity: 0.0755\n",
            "English: was, Hindi: था, Similarity: -0.0464\n",
            "English: was, Hindi: थी, Similarity: 0.0072\n",
            "English: for, Hindi: लिये, Similarity: -0.0317\n",
            "English: that, Hindi: उस, Similarity: -0.0120\n",
            "English: that, Hindi: कि, Similarity: -0.0811\n",
            "English: with, Hindi: साथ, Similarity: 0.0568\n",
            "English: from, Hindi: से, Similarity: 0.1069\n",
            "English: from, Hindi: इससे, Similarity: 0.0309\n",
            "English: this, Hindi: ये, Similarity: -0.1552\n",
            "English: this, Hindi: यह, Similarity: -0.1453\n",
            "English: this, Hindi: इस, Similarity: -0.2058\n",
            "English: his, Hindi: उसकी, Similarity: 0.0269\n",
            "English: his, Hindi: उसका, Similarity: 0.0216\n",
            "English: his, Hindi: उसके, Similarity: 0.0578\n",
            "English: not, Hindi: नही, Similarity: 0.0132\n",
            "English: not, Hindi: नहीं, Similarity: 0.0316\n",
            "English: are, Hindi: हैं, Similarity: -0.0422\n",
            "English: talk, Hindi: बात, Similarity: -0.0482\n",
            "English: which, Hindi: जिससे, Similarity: 0.0348\n",
            "English: also, Hindi: भी, Similarity: -0.0750\n",
            "English: has, Hindi: रै, Similarity: -0.0903\n",
            "English: were, Hindi: यहूद, Similarity: -0.0206\n",
            "English: but, Hindi: परन्तु, Similarity: 0.0722\n",
            "English: but, Hindi: लेकिन, Similarity: 0.0606\n",
            "English: but, Hindi: लेकीन, Similarity: 0.0225\n",
            "English: but, Hindi: मगर, Similarity: 0.0374\n",
            "English: but, Hindi: लकिन, Similarity: 0.0750\n",
            "English: one, Hindi: एक, Similarity: 0.0040\n",
            "English: new, Hindi: नया, Similarity: 0.1166\n",
            "English: new, Hindi: नई, Similarity: 0.0069\n",
            "English: first, Hindi: प्रथम, Similarity: -0.0359\n",
            "English: first, Hindi: पहली, Similarity: -0.0717\n",
            "English: first, Hindi: पहले, Similarity: -0.0037\n",
            "English: first, Hindi: पहला, Similarity: 0.0480\n",
            "English: page, Hindi: पृष्ठ, Similarity: -0.0693\n",
            "English: page, Hindi: पेज, Similarity: -0.1299\n",
            "English: you, Hindi: आपको, Similarity: 0.1092\n",
            "English: you, Hindi: आप, Similarity: 0.0384\n",
            "English: you, Hindi: तुम, Similarity: 0.0154\n",
            "English: they, Hindi: उन्होंने, Similarity: 0.0693\n",
            "English: they, Hindi: वे, Similarity: 0.0095\n",
            "English: had, Hindi: था, Similarity: -0.0679\n",
            "English: article, Hindi: लेख, Similarity: -0.0244\n",
            "English: article, Hindi: आलेख, Similarity: -0.0179\n",
            "English: who, Hindi: जिसने, Similarity: 0.0361\n",
            "English: who, Hindi: कौन, Similarity: 0.0981\n",
            "English: who, Hindi: जो, Similarity: 0.0258\n",
            "English: all, Hindi: सभी, Similarity: 0.0993\n",
            "English: all, Hindi: सब, Similarity: 0.0341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Additional) Ablation Study\n",
        "\n",
        "We now explore how the size of the bilingual lexicon affects the alignment of English and Hindi word embeddings and the quality of word translation.\n",
        "\n",
        "For this, we use Procrustes alignment and result is evaluated using precision@1, precision@5, and accuracy."
      ],
      "metadata": {
        "id": "SZTbOt9fo1aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_ablation_study(en_embeddings, hi_embeddings, lexicon_sizes=[5000, 10000]):\n",
        "    results = {}\n",
        "    all_pairs = load_bilingual_lexicon('en-hi.txt')\n",
        "\n",
        "    for size in lexicon_sizes:\n",
        "        print(f\"Performing alignment with lexicon size: {size}\")\n",
        "        en_hi_pairs = all_pairs[:size] # use a subset of lexicon\n",
        "        en_vecs, hi_vecs = extract_word_embeddings(en_hi_pairs, en_embeddings, hi_embeddings)\n",
        "        # Procrustes alignment\n",
        "        W = orthogonal_procrustes(en_vecs, hi_vecs)\n",
        "        # apply the learned mapping to all English word embeddings\n",
        "        aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
        "        translations = translate_words(aligned_en_embeddings, hi_embeddings, top_k=5,limit_size=2000)\n",
        "        test_dict = load_muse_test_dict('en-hi.txt')\n",
        "        precision_at_1, precision_at_5, accuracy = evaluate_translation(translations, test_dict)\n",
        "        results[size] = (precision_at_1, precision_at_5, accuracy)\n",
        "\n",
        "    return results\n",
        "\n",
        "ablation_results = perform_ablation_study(en_embeddings, hi_embeddings)\n",
        "\n",
        "for size, (p1, p5, acc) in ablation_results.items():\n",
        "    print(f\"Lexicon size: {size}\")\n",
        "    print(f\"  Precision@1: {p1:.4f}\")\n",
        "    print(f\"  Precision@5: {p5:.4f}\")\n",
        "    print(f\"  Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUUCwC1DoTs5",
        "outputId": "1f411e57-5cdc-428e-e324-92fbb61cd18b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing alignment with lexicon size: 5000\n",
            "Performing alignment with lexicon size: 10000\n",
            "Lexicon size: 5000\n",
            "  Precision@1: 0.4121\n",
            "  Precision@5: 0.7208\n",
            "  Accuracy: 0.7208\n",
            "Lexicon size: 10000\n",
            "  Precision@1: 0.3962\n",
            "  Precision@5: 0.7080\n",
            "  Accuracy: 0.7080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbVhGMjgpjUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}