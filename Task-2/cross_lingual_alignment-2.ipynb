{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook includes iterative Procrustes in addition to orthogonal Procrustes for embedding alignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG1Nb-ClhEPW"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "Firstly, we download pre-trained embeddings in English and Hindi and the bilingual dictionary from MUSE and prepare the data for alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcdM6HH5eygx",
        "outputId": "97a60272-0dce-4828-b2bd-442101f115b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-19 06:05:21--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.108, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G   191MB/s    in 11s     \n",
            "\n",
            "2025-04-19 06:05:32 (113 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download pre-trained English word embeddings\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr8mKbv5fLfD",
        "outputId": "753cd047-b498-4748-81d0-f25b37d4627d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-19 06:05:33--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.108, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1118942272 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘cc.hi.300.vec.gz’\n",
            "\n",
            "cc.hi.300.vec.gz    100%[===================>]   1.04G  53.5MB/s    in 8.8s    \n",
            "\n",
            "2025-04-19 06:05:41 (121 MB/s) - ‘cc.hi.300.vec.gz’ saved [1118942272/1118942272]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download pre-trained Hindi word embeddings\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABYl_zqqgY-0"
      },
      "source": [
        "We load top 100,000 embeddings from English and Hindi in decreasing order of frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_IjgGMlflKP",
        "outputId": "20f5c874-40a7-412c-e8fe-5b747c145139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 100000 English embeddings\n",
            "Loaded 100000 Hindi embeddings\n"
          ]
        }
      ],
      "source": [
        "# load the embeddings\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "def load_embeddings(file_path, top_n=100000):\n",
        "    embeddings = {}\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            if i > top_n:\n",
        "                break\n",
        "            tokens = line.decode('utf-8').strip().split(' ')\n",
        "            word = tokens[0]\n",
        "            vector = np.array(tokens[1:], dtype=np.float32)\n",
        "            vector = vector / np.linalg.norm(vector)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "en_embeddings = load_embeddings('cc.en.300.vec.gz', top_n=100000)\n",
        "hi_embeddings = load_embeddings('cc.hi.300.vec.gz', top_n=100000)\n",
        "\n",
        "print(f\"Loaded {len(en_embeddings)} English embeddings\")\n",
        "print(f\"Loaded {len(hi_embeddings)} Hindi embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCHz0m2Gf8bu",
        "outputId": "0740429e-b4ba-4acf-dfe4-9da809e18384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-19 06:06:03--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.51, 3.163.189.14, 3.163.189.96, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930856 (909K) [text/x-c++]\n",
            "Saving to: ‘en-hi.txt’\n",
            "\n",
            "en-hi.txt           100%[===================>] 909.04K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-19 06:06:03 (19.2 MB/s) - ‘en-hi.txt’ saved [930856/930856]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download english-hindi dictionary from MUSE\n",
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahXbJIfVgS3H",
        "outputId": "f7c0d068-d5d5-4cba-c9cb-f9b2c3de66b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('and', 'और'), ('was', 'था'), ('was', 'थी'), ('for', 'लिये'), ('that', 'उस'), ('that', 'कि'), ('with', 'साथ'), ('from', 'से'), ('from', 'इससे'), ('this', 'ये')]\n"
          ]
        }
      ],
      "source": [
        "# display English-Hindi pairs\n",
        "def load_bilingual_lexicon(file_path):\n",
        "    bilingual_dict = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            bilingual_dict.append((en_word, hi_word))\n",
        "    return bilingual_dict\n",
        "\n",
        "en_hi_pairs = load_bilingual_lexicon('en-hi.txt')\n",
        "print(en_hi_pairs[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2c3FJZ0g18l",
        "outputId": "a746888d-7f5c-41d2-c3f0-11b5f011975c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 18972 aligned word vectors.\n"
          ]
        }
      ],
      "source": [
        "# Extract word embeddings for bilingual word pairs\n",
        "import numpy as np\n",
        "\n",
        "def extract_word_embeddings(bilingual_pairs, en_embeddings, hi_embeddings):\n",
        "    en_vecs = []\n",
        "    hi_vecs = []\n",
        "\n",
        "    for en_word, hi_word in bilingual_pairs:\n",
        "        if en_word in en_embeddings and hi_word in hi_embeddings:\n",
        "            en_vecs.append(en_embeddings[en_word])\n",
        "            hi_vecs.append(hi_embeddings[hi_word])\n",
        "\n",
        "    en_vecs = np.array(en_vecs)\n",
        "    hi_vecs = np.array(hi_vecs)\n",
        "\n",
        "    return en_vecs, hi_vecs\n",
        "\n",
        "en_vecs, hi_vecs = extract_word_embeddings(en_hi_pairs, en_embeddings, hi_embeddings)\n",
        "print(f\"Extracted {en_vecs.shape[0]} aligned word vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmOp7xbihW7F"
      },
      "source": [
        "## Cross-Lingual Alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxRjLXzthe7s"
      },
      "source": [
        "### Procrustes Alignment\n",
        "\n",
        "We perform orthogonal Procrustes alignment to learn a mapping from X to Y, where X is numpy array having source language word embeddings (English) and Y is numpy array having target language word embeddings (Hindi)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnxrwPCohBCN",
        "outputId": "fbaa8786-26b1-44c0-8fc4-34fe7bcf9773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orthogonal mapping matrix learned.\n"
          ]
        }
      ],
      "source": [
        "def orthogonal_procrustes(X, Y):\n",
        "    X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    Y = Y / np.linalg.norm(Y, axis=1, keepdims=True)\n",
        "    M = np.dot(X.T, Y)\n",
        "\n",
        "    U, _, Vt = np.linalg.svd(M)\n",
        "    W = np.dot(U, Vt)\n",
        "\n",
        "    return W\n",
        "\n",
        "W = orthogonal_procrustes(en_vecs, hi_vecs)\n",
        "\n",
        "print(\"Orthogonal mapping matrix learned.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imls2ynAhvqi",
        "outputId": "3534a34a-9edc-4e78-b912-ea1daeed11b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aligned 100000 English embeddings into the Hindi space.\n"
          ]
        }
      ],
      "source": [
        "# Apply learned mapping to the source language embeddings\n",
        "\n",
        "def apply_mapping(embeddings, W):\n",
        "    mapped_embeddings = {}\n",
        "    for word, vec in embeddings.items():\n",
        "        mapped_vec = np.dot(vec, W)\n",
        "        mapped_vec = mapped_vec / np.linalg.norm(mapped_vec) # normalize the mapped vector\n",
        "        mapped_embeddings[word] = mapped_vec\n",
        "    return mapped_embeddings\n",
        "\n",
        "aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
        "\n",
        "print(f\"Aligned {len(aligned_en_embeddings)} English embeddings into the Hindi space.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNaux3ZI7i0a"
      },
      "source": [
        "### Iterative Procrustes\n",
        "\n",
        "Run for 2 iterations on a subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kggsZl4n7m2W"
      },
      "outputs": [],
      "source": [
        "def iterative_procrustes(X, Y):\n",
        "    hi_words = list(hi_embeddings.keys())\n",
        "    hi_vecs = np.array(list(hi_embeddings.values()))\n",
        "\n",
        "    W = orthogonal_procrustes(X, Y)\n",
        "\n",
        "    for i in range(3):\n",
        "        print(f\"Iteration {i+1}/3\")\n",
        "\n",
        "        aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
        "\n",
        "        new_pairs = []\n",
        "\n",
        "        en_words = list(en_embeddings.keys())[:5000]\n",
        "        for en_word in en_words:\n",
        "            en_vec = aligned_en_embeddings[en_word]\n",
        "            similarities = cosine_similarity([en_vec], hi_vecs)[0]\n",
        "            best_idx = np.argmax(similarities)\n",
        "            best_hi_word = hi_words[best_idx]\n",
        "            new_pairs.append((en_word, best_hi_word))\n",
        "\n",
        "        print(f\"  Found {len(new_pairs)} new pairs\")\n",
        "\n",
        "        X_new, Y_new = extract_word_embeddings(new_pairs, en_embeddings, hi_embeddings)\n",
        "\n",
        "        print(f\"  Using {len(X_new)} valid pairs for alignment\")\n",
        "\n",
        "        if len(X_new) == 0:\n",
        "            print(\"  No valid pairs found, stopping iterations\")\n",
        "            break\n",
        "\n",
        "        W = orthogonal_procrustes(X_new, Y_new)\n",
        "\n",
        "    return W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XEHzETi-xn-",
        "outputId": "911fe1db-5c8b-40c7-97ed-884341c7202f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1/3\n",
            "  Found 5000 new pairs\n",
            "  Using 5000 valid pairs for alignment\n",
            "Iteration 2/3\n",
            "  Found 5000 new pairs\n",
            "  Using 5000 valid pairs for alignment\n"
          ]
        }
      ],
      "source": [
        "W_iter = iterative_procrustes(en_vecs, hi_vecs)\n",
        "aligned_en_iter = apply_mapping(en_embeddings, W_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0-87VNhiF0V"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YgH2b3DiK6-"
      },
      "source": [
        "### Word Translation\n",
        "\n",
        "We now perform word translation where we translate a limited number of words from English to Hindi using the aligned embeddings. I have limited size of en_words to 2000 since I previously experimented with 5000 words and session crashed terminated after execution of > 2 hours.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sha5FRivh8Uv",
        "outputId": "c474349e-57be-4264-c42d-062a3a9f0d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: , -> Hindi: [',', 'और', 'कि', 'हैं', '।']\n",
            "English: the -> Hindi: ['में', 'पहले', 'सबसे', 'अपने', 'जिस']\n",
            "English: . -> Hindi: ['तो', 'ही', '*', 'आज', '.']\n",
            "English: and -> Hindi: ['तथा', 'साथ', 'एवं', 'और', 'हैं']\n",
            "English: to -> Hindi: ['करके', 'करने', 'करना', 'करें', 'करते']\n",
            "English: of -> Hindi: ['में', 'प्रति', 'तथा', 'सबसे', 'अधीन']\n",
            "English: a -> Hindi: ['बड़ा', 'दूसरा', 'बड़ा', 'पहला', 'छोटा']\n",
            "English: </s> -> Hindi: ['▲', 'è', 'bss', 'pd', '▓']\n",
            "English: in -> Hindi: ['में', 'बाहर', 'जहां', 'मे', 'लाने']\n",
            "English: is -> Hindi: ['है', 'यह', 'होता', 'क्योंकि', 'जो']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def translate_words(aligned_en_embeddings, hi_embeddings, top_k=5, limit_size=None):\n",
        "    translations = {}\n",
        "    hi_words = list(hi_embeddings.keys())\n",
        "    hi_vecs = np.array(list(hi_embeddings.values()))\n",
        "\n",
        "    en_words = list(aligned_en_embeddings.keys())\n",
        "    if limit_size is not None:\n",
        "        en_words = en_words[:limit_size]\n",
        "\n",
        "    for en_word in en_words:\n",
        "        en_vec = aligned_en_embeddings[en_word]\n",
        "        en_vec = en_vec / np.linalg.norm(en_vec) # calculate cosine similarity\n",
        "        hi_vecs_norm = hi_vecs / np.linalg.norm(hi_vecs, axis=1, keepdims=True)\n",
        "        similarities = cosine_similarity([en_vec], hi_vecs_norm).flatten()\n",
        "        nearest_idxs = similarities.argsort()[-top_k:][::-1] # get top_k most similar Hindi words\n",
        "        nearest_words = [hi_words[i] for i in nearest_idxs]\n",
        "\n",
        "        translations[en_word] = nearest_words\n",
        "\n",
        "    return translations\n",
        "\n",
        "limit_size = 2000\n",
        "translations = translate_words(aligned_en_embeddings, hi_embeddings, top_k=5, limit_size=limit_size)\n",
        "\n",
        "for en_word, hi_words in list(translations.items())[:10]:\n",
        "    print(f\"English: {en_word} -> Hindi: {hi_words}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acuGiy2_l5qn"
      },
      "source": [
        "### Translation accuracy and Precision using MUSE dictionary\n",
        "\n",
        "We calculate Translation accuracy, P@1 and P@5 to evaluate the aligned embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZRa6t2_xi881"
      },
      "outputs": [],
      "source": [
        "def load_muse_test_dict(file_path):\n",
        "    test_dict = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            words = line.strip().split()\n",
        "            if len(words) == 2:\n",
        "                test_dict[words[0]] = words[1]\n",
        "    return test_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTPXTv6MlpIX"
      },
      "source": [
        "#### Orthogonal Procrustes Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5dNbnxlnbZE",
        "outputId": "5ca2fe83-620a-4b4d-9b7a-63df9d60c9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@1: 0.3827\n",
            "Precision@5: 0.6675\n",
            "Accuracy: 0.6675\n"
          ]
        }
      ],
      "source": [
        "def evaluate_translation(translations, test_dict, top_k=5):\n",
        "    true_positives_at_1 = 0\n",
        "    true_positives_at_5 = 0\n",
        "    false_positives_at_1 = 0\n",
        "    false_positives_at_5 = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for en_word, correct_hi_word in test_dict.items():\n",
        "        predicted_hi_words = translations.get(en_word, [])\n",
        "\n",
        "        if len(predicted_hi_words) > 0:\n",
        "            total_predictions += 1\n",
        "\n",
        "            # calculate Precision@1\n",
        "            if correct_hi_word == predicted_hi_words[0]:\n",
        "                true_positives_at_1 += 1\n",
        "                correct_predictions += 1\n",
        "            else:\n",
        "                false_positives_at_1 += 1\n",
        "\n",
        "            # calculate Precision@5\n",
        "            if correct_hi_word in predicted_hi_words[:top_k]:\n",
        "                true_positives_at_5 += 1\n",
        "\n",
        "                # Only count for accuracy if it hasn't been counted for Precision@1\n",
        "                if correct_hi_word != predicted_hi_words[0]:\n",
        "                    correct_predictions += 1\n",
        "            else:\n",
        "                false_positives_at_5 += 1\n",
        "\n",
        "    precision_at_1 = true_positives_at_1 / (true_positives_at_1 + false_positives_at_1) if (true_positives_at_1 + false_positives_at_1) > 0 else 0\n",
        "    precision_at_5 = true_positives_at_5 / (true_positives_at_5 + false_positives_at_5) if (true_positives_at_5 + false_positives_at_5) > 0 else 0\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "    return precision_at_1, precision_at_5, accuracy\n",
        "\n",
        "test_dict = load_muse_test_dict('en-hi.txt')\n",
        "precision_at_1, precision_at_5, accuracy = evaluate_translation(translations, test_dict)\n",
        "\n",
        "print(f\"Precision@1: {precision_at_1:.4f}\")\n",
        "print(f\"Precision@5: {precision_at_5:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SCaKYy8luj-"
      },
      "source": [
        "#### Iterative Procrustes Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOZmSeNjlYEY",
        "outputId": "f9312203-b90d-4e70-cf11-7d4cda0c3c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iterative Procrustes Results:\n",
            "Precision@1: 0.3652\n",
            "Precision@5: 0.6006\n",
            "Accuracy: 0.6006\n"
          ]
        }
      ],
      "source": [
        "translations_iter = translate_words(aligned_en_iter, hi_embeddings, top_k=5, limit_size=2000)\n",
        "\n",
        "# evaluate using MUSE dictionary\n",
        "test_dict = load_muse_test_dict('en-hi.txt')\n",
        "precision_at_1, precision_at_5, accuracy = evaluate_translation(translations_iter, test_dict)\n",
        "\n",
        "print(f\"Iterative Procrustes Results:\")\n",
        "print(f\"Precision@1: {precision_at_1:.4f}\")\n",
        "print(f\"Precision@5: {precision_at_5:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSEbctpynv-l"
      },
      "source": [
        "### Cosine similarity\n",
        "Now, we calculate cosine similarity to measure the similarity between the embeddings in English and in Hindi, thereby getting a quantitative idea about alignment quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8frH25oEnpQY",
        "outputId": "d8168b64-b727-4b2b-d112-c8819d2a8b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: and, Hindi: और, Similarity: 0.0755\n",
            "English: was, Hindi: था, Similarity: -0.0464\n",
            "English: was, Hindi: थी, Similarity: 0.0072\n",
            "English: for, Hindi: लिये, Similarity: -0.0317\n",
            "English: that, Hindi: उस, Similarity: -0.0120\n",
            "English: that, Hindi: कि, Similarity: -0.0811\n",
            "English: with, Hindi: साथ, Similarity: 0.0568\n",
            "English: from, Hindi: से, Similarity: 0.1069\n",
            "English: from, Hindi: इससे, Similarity: 0.0309\n",
            "English: this, Hindi: ये, Similarity: -0.1552\n",
            "English: this, Hindi: यह, Similarity: -0.1453\n",
            "English: this, Hindi: इस, Similarity: -0.2058\n",
            "English: his, Hindi: उसकी, Similarity: 0.0269\n",
            "English: his, Hindi: उसका, Similarity: 0.0216\n",
            "English: his, Hindi: उसके, Similarity: 0.0578\n",
            "English: not, Hindi: नही, Similarity: 0.0132\n",
            "English: not, Hindi: नहीं, Similarity: 0.0316\n",
            "English: are, Hindi: हैं, Similarity: -0.0422\n",
            "English: talk, Hindi: बात, Similarity: -0.0482\n",
            "English: which, Hindi: जिससे, Similarity: 0.0348\n",
            "English: also, Hindi: भी, Similarity: -0.0750\n",
            "English: has, Hindi: रै, Similarity: -0.0903\n",
            "English: were, Hindi: यहूद, Similarity: -0.0206\n",
            "English: but, Hindi: परन्तु, Similarity: 0.0722\n",
            "English: but, Hindi: लेकिन, Similarity: 0.0606\n",
            "English: but, Hindi: लेकीन, Similarity: 0.0225\n",
            "English: but, Hindi: मगर, Similarity: 0.0374\n",
            "English: but, Hindi: लकिन, Similarity: 0.0750\n",
            "English: one, Hindi: एक, Similarity: 0.0040\n",
            "English: new, Hindi: नया, Similarity: 0.1166\n",
            "English: new, Hindi: नई, Similarity: 0.0069\n",
            "English: first, Hindi: प्रथम, Similarity: -0.0359\n",
            "English: first, Hindi: पहली, Similarity: -0.0717\n",
            "English: first, Hindi: पहले, Similarity: -0.0037\n",
            "English: first, Hindi: पहला, Similarity: 0.0480\n",
            "English: page, Hindi: पृष्ठ, Similarity: -0.0693\n",
            "English: page, Hindi: पेज, Similarity: -0.1299\n",
            "English: you, Hindi: आपको, Similarity: 0.1092\n",
            "English: you, Hindi: आप, Similarity: 0.0384\n",
            "English: you, Hindi: तुम, Similarity: 0.0154\n",
            "English: they, Hindi: उन्होंने, Similarity: 0.0693\n",
            "English: they, Hindi: वे, Similarity: 0.0095\n",
            "English: had, Hindi: था, Similarity: -0.0679\n",
            "English: article, Hindi: लेख, Similarity: -0.0244\n",
            "English: article, Hindi: आलेख, Similarity: -0.0179\n",
            "English: who, Hindi: जिसने, Similarity: 0.0361\n",
            "English: who, Hindi: कौन, Similarity: 0.0981\n",
            "English: who, Hindi: जो, Similarity: 0.0258\n",
            "English: all, Hindi: सभी, Similarity: 0.0993\n",
            "English: all, Hindi: सब, Similarity: 0.0341\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_cosine_similarities(en_word_pairs, en_embeddings, hi_embeddings, num_pairs=50):\n",
        "    similarities = {}\n",
        "    count = 0\n",
        "\n",
        "    for en_word, hi_word in en_word_pairs:\n",
        "        if en_word in en_embeddings and hi_word in hi_embeddings:\n",
        "            en_vec = en_embeddings[en_word]\n",
        "            hi_vec = hi_embeddings[hi_word]\n",
        "            en_vec = en_vec / np.linalg.norm(en_vec)\n",
        "            hi_vec = hi_vec / np.linalg.norm(hi_vec)\n",
        "            similarity = cosine_similarity([en_vec], [hi_vec])[0][0]\n",
        "            similarities[(en_word, hi_word)] = similarity\n",
        "\n",
        "            count += 1\n",
        "            if count >= num_pairs:\n",
        "                break\n",
        "\n",
        "    return similarities\n",
        "\n",
        "cosine_similarities = compute_cosine_similarities(en_hi_pairs, en_embeddings, hi_embeddings, num_pairs=50) # limit number of pairs to 50\n",
        "for (en_word, hi_word), similarity in cosine_similarities.items():\n",
        "    print(f\"English: {en_word}, Hindi: {hi_word}, Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZTbOt9fo1aW"
      },
      "source": [
        "### (Additional) Ablation Study\n",
        "\n",
        "We now explore how the size of the bilingual lexicon affects the alignment of English and Hindi word embeddings and the quality of word translation.\n",
        "\n",
        "For this, we use Procrustes alignment and result is evaluated using precision@1, precision@5, and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUUCwC1DoTs5",
        "outputId": "1f411e57-5cdc-428e-e324-92fbb61cd18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing alignment with lexicon size: 5000\n",
            "Performing alignment with lexicon size: 10000\n",
            "Lexicon size: 5000\n",
            "  Precision@1: 0.4121\n",
            "  Precision@5: 0.7208\n",
            "  Accuracy: 0.7208\n",
            "Lexicon size: 10000\n",
            "  Precision@1: 0.3962\n",
            "  Precision@5: 0.7080\n",
            "  Accuracy: 0.7080\n"
          ]
        }
      ],
      "source": [
        "def perform_ablation_study(en_embeddings, hi_embeddings, lexicon_sizes=[5000, 10000]):\n",
        "    results = {}\n",
        "    all_pairs = load_bilingual_lexicon('en-hi.txt')\n",
        "\n",
        "    for size in lexicon_sizes:\n",
        "        print(f\"Performing alignment with lexicon size: {size}\")\n",
        "        en_hi_pairs = all_pairs[:size] # use a subset of lexicon\n",
        "        en_vecs, hi_vecs = extract_word_embeddings(en_hi_pairs, en_embeddings, hi_embeddings)\n",
        "        # Procrustes alignment\n",
        "        W = orthogonal_procrustes(en_vecs, hi_vecs)\n",
        "        # apply the learned mapping to all English word embeddings\n",
        "        aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
        "        translations = translate_words(aligned_en_embeddings, hi_embeddings, top_k=5,limit_size=2000)\n",
        "        test_dict = load_muse_test_dict('en-hi.txt')\n",
        "        precision_at_1, precision_at_5, accuracy = evaluate_translation(translations, test_dict)\n",
        "        results[size] = (precision_at_1, precision_at_5, accuracy)\n",
        "\n",
        "    return results\n",
        "\n",
        "ablation_results = perform_ablation_study(en_embeddings, hi_embeddings)\n",
        "\n",
        "for size, (p1, p5, acc) in ablation_results.items():\n",
        "    print(f\"Lexicon size: {size}\")\n",
        "    print(f\"  Precision@1: {p1:.4f}\")\n",
        "    print(f\"  Precision@5: {p5:.4f}\")\n",
        "    print(f\"  Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cOVg8feKOfE"
      },
      "source": [
        "Use Iterative Procrustes alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSBg8EpyJpqy",
        "outputId": "7eabd8c8-a42a-42e0-cdd3-6d25263afe0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing alignment with lexicon size: 5000\n",
            "Iteration 1/3\n",
            "  Found 5000 new pairs\n",
            "  Using 5000 valid pairs for alignment\n",
            "Iteration 2/3\n",
            "  Found 5000 new pairs\n",
            "  Using 5000 valid pairs for alignment\n",
            "Performing alignment with lexicon size: 10000\n",
            "Iteration 1/3\n",
            "  Found 5000 new pairs\n",
            "  Using 5000 valid pairs for alignment\n",
            "Iteration 2/3\n",
            "  Found 5000 new pairs\n",
            "  Using 5000 valid pairs for alignment\n",
            "Lexicon size: 5000\n",
            "  Precision@1: 0.3827\n",
            "  Precision@5: 0.6675\n",
            "  Accuracy: 0.6675\n",
            "Lexicon size: 10000\n",
            "  Precision@1: 0.3827\n",
            "  Precision@5: 0.6675\n",
            "  Accuracy: 0.6675\n"
          ]
        }
      ],
      "source": [
        "def perform_ablation_study(en_embeddings, hi_embeddings, lexicon_sizes=[5000, 10000]):\n",
        "    results = {}\n",
        "    all_pairs = load_bilingual_lexicon('en-hi.txt')\n",
        "\n",
        "    for size in lexicon_sizes:\n",
        "        print(f\"Performing alignment with lexicon size: {size}\")\n",
        "        en_hi_pairs = all_pairs[:size] # use a subset of lexicon\n",
        "        en_vecs, hi_vecs = extract_word_embeddings(en_hi_pairs, en_embeddings, hi_embeddings)\n",
        "        # Iterative Procrustes alignment\n",
        "        W_iterative = iterative_procrustes(en_vecs, hi_vecs)\n",
        "        # apply the learned mapping to all English word embeddings\n",
        "        aligned_en_iterative = apply_mapping(en_embeddings, W)\n",
        "        translations_iterative = translate_words(aligned_en_embeddings, hi_embeddings, top_k=5,limit_size=2000)\n",
        "        test_dict = load_muse_test_dict('en-hi.txt')\n",
        "        precision_at_1, precision_at_5, accuracy = evaluate_translation(translations_iterative, test_dict)\n",
        "        results[size] = (precision_at_1, precision_at_5, accuracy)\n",
        "\n",
        "    return results\n",
        "\n",
        "ablation_results = perform_ablation_study(en_embeddings, hi_embeddings)\n",
        "\n",
        "for size, (p1, p5, acc) in ablation_results.items():\n",
        "    print(f\"Lexicon size: {size}\")\n",
        "    print(f\"  Precision@1: {p1:.4f}\")\n",
        "    print(f\"  Precision@5: {p5:.4f}\")\n",
        "    print(f\"  Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOEwgYAJK-Ep"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
