'''
This script is used to evaluate the embeddings generated by the SVD reduced co-occurrence matrix and pre-trained embeddings generated by GloVe and Word2Vec. 
The metrics used are Spearman's rank correlation coefficient and cosine similarity. 
Both the embeddings are also extrinsicly evaluated using the word similarity task, where the words similar to the word 'computer' are analyzed from all embeddings along with similarity scores.
'''

import pickle
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from scipy import stats
import os
from scipy.sparse.linalg import svds
import random
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from scipy import sparse
import time

def load_vocab(vocab_path):
    print(f"Loading vocabulary from {vocab_path}...")
    vocab = []
    with open(vocab_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.startswith('#'):
                continue
            words = line.strip().split()
            if words:
                word = words[0]
                vocab.append(word)
    
    word2id = {word: idx for idx, word in enumerate(vocab)}
    id2word = {idx: word for idx, word in enumerate(vocab)}
    print(f"Loaded vocabulary with {len(vocab)} words")
    return vocab, word2id, id2word

def load_co_occurrence(matrix_path):
    print(f"Loading co-occurrence matrix from {matrix_path}...")
    with open(matrix_path, 'rb') as f:
        data = pickle.load(f)
    
    if isinstance(data, dict):
        print("Co-occurrence data is in dictionary format")
        if 'matrix' in data:
            matrix = data['matrix']
            print(f"Matrix shape: {matrix.shape}")
        else:
            print("No 'matrix' key found in co-occurrence data")
            matrix = None
    else:
        matrix = data
        print(f"Matrix shape: {matrix.shape}")
    
    return matrix

def load_embeddings(embedding_path):
    print(f"Loading embeddings from {embedding_path}...")
    with open(embedding_path, 'rb') as f:
        embeddings = pickle.load(f)
    
    if isinstance(embeddings, dict):
        print(f"Embeddings are in dictionary format with {len(embeddings)} entries")
        dims = [len(vec) for vec in embeddings.values() if hasattr(vec, '__len__')]
        if dims:
            print(f"Embedding dimension: {dims[0]}")
    else:
        print(f"Embeddings format: {type(embeddings)}")
        if hasattr(embeddings, 'shape'):
            print(f"Embeddings shape: {embeddings.shape}")
    
    return embeddings

def extract_embeddings_from_co_occurrence(matrix, dim=300):
    print(f"Requested SVD embeddings with dimension {dim}...")
    start_time = time.time()
    
    if not sparse.issparse(matrix):
        print("Converting to sparse matrix format...")
        matrix = sparse.csr_matrix(matrix)
    
    log_matrix = matrix.copy()
    log_matrix.data = np.log1p(log_matrix.data)
    
    max_possible_dim = min(log_matrix.shape) - 1
    if dim >= max_possible_dim:
        print(f"Requested dimension {dim} is too large. Reducing to {max_possible_dim}")
        dim = max_possible_dim
    
    print(f"Computing sparse SVD with dimension {dim} (this may take a while)...")
    U, s, Vt = svds(log_matrix, k=dim)
    
    # sort the singular values
    idx = np.argsort(s)[::-1]
    s = s[idx]
    U = U[:, idx]
    embeddings = U * np.sqrt(s)
    
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    normalized_embeddings = embeddings / norms
    
    elapsed_time = time.time() - start_time
    print(f"SVD completed in {elapsed_time:.2f} seconds")
    print(f"Created SVD embeddings with shape: {normalized_embeddings.shape}")
    
    return normalized_embeddings

def load_simlex(simlex_path):
    print(f"Loading SimLex-999 from {simlex_path}...")
    word_pairs = []
    human_scores = []
    
    with open(simlex_path, 'r', encoding='utf-8') as f:
        first_line = f.readline().strip()
        if not any(c.isdigit() for c in first_line.split()[-1]):
            print("Skipping header line")
        else:

            parts = first_line.split()
            if len(parts) >= 3:
                word1, word2, score = parts[0], parts[1], float(parts[2])
                word_pairs.append((word1, word2))
                human_scores.append(score)
                
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 3:
                word1, word2, score = parts[0], parts[1], float(parts[2])
                word_pairs.append((word1, word2))
                human_scores.append(score)
    
    print(f"Loaded {len(word_pairs)} word pairs from SimLex-999")
    return word_pairs, human_scores

def evaluate_word_pairs(embeddings, word_pairs, human_scores, word2id=None):
    embedding_scores = []
    valid_pairs = []
    valid_human_scores = []
    is_dict_format = isinstance(embeddings, dict)
    
    for (word1, word2), human_score in zip(word_pairs, human_scores):
        if is_dict_format:
            if word1 in embeddings and word2 in embeddings:
                vector1 = embeddings[word1]
                vector2 = embeddings[word2]
                vector1 = vector1 / np.linalg.norm(vector1)
                vector2 = vector2 / np.linalg.norm(vector2)
                similarity = np.dot(vector1, vector2)
                embedding_scores.append(similarity)
                valid_pairs.append((word1, word2))
                valid_human_scores.append(human_score)
        else:
            if word2id and word1 in word2id and word2 in word2id:
                vector1 = embeddings[word2id[word1]]
                vector2 = embeddings[word2id[word2]]
                vector1 = vector1 / np.linalg.norm(vector1)
                vector2 = vector2 / np.linalg.norm(vector2)
                
                similarity = np.dot(vector1, vector2)                
                embedding_scores.append(similarity)
                valid_pairs.append((word1, word2))
                valid_human_scores.append(human_score)
    
    if len(embedding_scores) > 0:
        correlation, p_value = stats.spearmanr(embedding_scores, valid_human_scores)
        result = {
            'correlation': correlation,
            'p_value': p_value,
            'num_pairs': len(embedding_scores),
            'missing_words': len(word_pairs) - len(embedding_scores)
        }
        return result
    else:
        return {'error': 'No valid word pairs found'}
    
    
def find_similar_words(word, embeddings, word2id=None, id2word=None, n=10):
    is_dict_format = isinstance(embeddings, dict)
    
    if is_dict_format:
        if word not in embeddings:
            print(f"Word '{word}' not in embeddings")
            return []
            
        word_vector = embeddings[word]
        similarities = []
        for w, vec in embeddings.items():
            if w != word:
                sim = cosine_similarity([word_vector], [vec])[0][0]
                similarities.append((w, sim))
        
        most_similar = sorted(similarities, key=lambda x: x[1], reverse=True)[:n]
    else:
        if word2id is None or id2word is None:
            print("word2id and id2word mappings required for matrix-format embeddings")
            return []
            
        if word not in word2id:
            print(f"Word '{word}' not in vocabulary")
            return []
            
        word_id = word2id[word]
        word_vector = embeddings[word_id].reshape(1, -1)
        
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        normalized_embeddings = embeddings / norms
        normalized_word_vector = word_vector / np.linalg.norm(word_vector)
        
        similarities = np.dot(normalized_embeddings, normalized_word_vector.T).flatten()
        most_similar = [(id2word[i], similarities[i]) 
                        for i in similarities.argsort()[::-1]
                        if i != word_id][:n]
    
    return most_similar

def visualize_embeddings(embeddings, words, word2id=None, method='pca', title=None):
    is_dict_format = isinstance(embeddings, dict)
    
    if is_dict_format:
        words = [w for w in words if w in embeddings]
        if not words:
            print("No words found in embeddings")
            return
            
        word_vectors = np.array([embeddings[w] for w in words])
    else:
        if word2id is None:
            print("word2id mapping required for matrix-format embeddings")
            return
            
        words = [w for w in words if w in word2id]
        if not words:
            print("No words found in vocabulary")
            return
            
        word_ids = [word2id[w] for w in words]
        word_vectors = embeddings[word_ids]
    
    if method.lower() == 'tsne':
        perplexity = min(30, len(words) - 1)
        reducer = TSNE(n_components=2, random_state=42, perplexity=perplexity)
        reduced_vecs = reducer.fit_transform(word_vectors)
        method_name = "t-SNE"
    else: 
        reducer = PCA(n_components=2)
        reduced_vecs = reducer.fit_transform(word_vectors)
        method_name = "PCA"
    
    plt.figure(figsize=(12, 10))
    plt.scatter(reduced_vecs[:, 0], reduced_vecs[:, 1], alpha=0.7)
    
    for i, word in enumerate(words):
        plt.annotate(word, (reduced_vecs[i, 0], reduced_vecs[i, 1]), fontsize=12)
    
    if title:
        plt.title(f'{title} ({method_name})')
    else:
        plt.title(f'2D visualization of word embeddings using {method_name}')
        
    plt.tight_layout()
    plt.grid(alpha=0.3)
    return plt

def plot_comparison_results(results):
    methods = list(results.keys())
    correlations = [results[method]['correlation'] for method in methods]
    
    plt.figure(figsize=(10, 6))
    bars = plt.bar(methods, correlations, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
    
    for bar, corr in zip(bars, correlations):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{corr:.4f}', ha='center', va='bottom', fontsize=12)
    
    plt.title('Spearman Correlation with SimLex-999')
    plt.ylabel('Correlation')
    plt.ylim(0, max(correlations) + 0.1)
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    return plt

def main():
    vocab_path = 'data/vocab_file.txt'
    vocab, word2id, id2word = load_vocab(vocab_path)
    
    matrix_path = 'models/co-occurrence-symmetry/cooc_matrix_w5.pkl'
    cooc_matrix = load_co_occurrence(matrix_path)
    
    word2vec_path = 'embeddings/word2vec_embeddings.pkl'
    glove_path = 'embeddings/glove_6B_300d.pkl'
    
    word2vec_embeddings = load_embeddings(word2vec_path)
    glove_embeddings = load_embeddings(glove_path)
    
    svd_embeddings = extract_embeddings_from_co_occurrence(cooc_matrix, dim=300)    
    simlex_path = 'data/simlex_full.txt'
    word_pairs, human_scores = load_simlex(simlex_path)
    
    print("\nEvaluating embeddings on SimLex-999...")
    
    svd_result = evaluate_word_pairs(svd_embeddings, word_pairs, human_scores, word2id)
    word2vec_result = evaluate_word_pairs(word2vec_embeddings, word_pairs, human_scores)
    glove_result = evaluate_word_pairs(glove_embeddings, word_pairs, human_scores)
    
    print("\nSVD Embeddings:")
    print(f"  Spearman correlation: {svd_result['correlation']:.4f}")
    print(f"  p-value: {svd_result['p_value']:.4e}")
    print(f"  Valid pairs: {svd_result['num_pairs']} (missing: {svd_result['missing_words']})")
    
    print("\nWord2Vec Embeddings:")
    print(f"  Spearman correlation: {word2vec_result['correlation']:.4f}")
    print(f"  p-value: {word2vec_result['p_value']:.4e}")
    print(f"  Valid pairs: {word2vec_result['num_pairs']} (missing: {word2vec_result['missing_words']})")
    
    print("\nGloVe Embeddings:")
    print(f"  Spearman correlation: {glove_result['correlation']:.4f}")
    print(f"  p-value: {glove_result['p_value']:.4e}")
    print(f"  Valid pairs: {glove_result['num_pairs']} (missing: {glove_result['missing_words']})")
    
    results = {
        'SVD': svd_result,
        'Word2Vec': word2vec_result,
        'GloVe': glove_result
    }
    
    best_method = max(results, key=lambda x: results[x]['correlation'])
    print(f"\nBest embedding method: {best_method} (correlation: {results[best_method]['correlation']:.4f})")
    
    comparison_plot = plot_comparison_results(results) # plot comparison results
    comparison_plot.savefig('plots/embedding_comparison.png', dpi=300, bbox_inches='tight')
    print("Saved comparison plot to plots/embedding_comparison.png")
    
    # sample 30 words from SimLex for visualization
    simlex_words = list(set([word1 for word1, _ in word_pairs] + [word2 for _, word2 in word_pairs]))
    if len(simlex_words) > 30:
        visualize_words_large = random.sample(simlex_words, 30)
    else:
        visualize_words_large = simlex_words

    print(f"\nVisualizing 30 random words from SimLex: {', '.join(visualize_words_large)}")

    # visualize embeddings with 30 words
    pca_svd_large = visualize_embeddings(svd_embeddings, visualize_words_large, word2id, 'pca', 'SVD Embeddings (30 words)')
    pca_svd_large.savefig('plots/svd_pca_large.png', dpi=300, bbox_inches='tight')

    pca_word2vec_large = visualize_embeddings(word2vec_embeddings, visualize_words_large, None, 'pca', 'Word2Vec Embeddings (30 words)')
    pca_word2vec_large.savefig('plots/word2vec_pca_large.png', dpi=300, bbox_inches='tight')

    pca_glove_large = visualize_embeddings(glove_embeddings, visualize_words_large, None, 'pca', 'GloVe Embeddings (30 words)')
    pca_glove_large.savefig('plots/glove_pca_large.png', dpi=300, bbox_inches='tight')
    
    tsne_svd = visualize_embeddings(svd_embeddings, visualize_words_large, word2id, 'tsne', 'SVD Embeddings')
    tsne_svd.savefig('plots/svd_tsne.png', dpi=300, bbox_inches='tight')
    
    tsne_word2vec = visualize_embeddings(word2vec_embeddings, visualize_words_large, None, 'tsne', 'Word2Vec Embeddings')
    tsne_word2vec.savefig('plots/word2vec_tsne.png', dpi=300, bbox_inches='tight')
    
    tsne_glove = visualize_embeddings(glove_embeddings, visualize_words_large, None, 'tsne', 'GloVe Embeddings')
    tsne_glove.savefig('plots/glove_tsne.png', dpi=300, bbox_inches='tight')

    print("Saved visualization plots to plots/ directory")

    
    # print sample similar words
    test_word = 'computer'
    print(f"\nFinding words similar to '{test_word}':")
    
    svd_similar = find_similar_words(test_word, svd_embeddings, word2id, id2word, 5)
    if svd_similar:
        print("\nSVD - Similar words:")
        for word, sim in svd_similar:
            print(f"  {word}: {sim:.4f}")
    
    word2vec_similar = find_similar_words(test_word, word2vec_embeddings, None, None, 5)
    if word2vec_similar:
        print("\nWord2Vec - Similar words:")
        for word, sim in word2vec_similar:
            print(f"  {word}: {sim:.4f}")
    
    glove_similar = find_similar_words(test_word, glove_embeddings, None, None, 5)
    if glove_similar:
        print("\nGloVe - Similar words:")
        for word, sim in glove_similar:
            print(f"  {word}: {sim:.4f}")

if __name__ == "__main__":
    os.makedirs('plots', exist_ok=True)
    main()